{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH7mpzC_9KJa"
      },
      "outputs": [],
      "source": [
        "# /Tuebingen_Neckarfront.jpg\n",
        "# /vangogh_starry_night.jpg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch의 핵심 라이브러리를 불러옴.\n",
        "import torch\n",
        "# PyTorch의 자동 미분 기능(Autograd)을 위한 Variable 클래스를 불러옴. (최신 PyTorch에서는 텐서가 대체함)\n",
        "from torch.autograd import Variable\n",
        "# 신경망 레이어(nn) 모듈을 불러옴.\n",
        "import torch.nn as nn\n",
        "# 함수형 신경망 연산(F) 모듈을 불러옴.\n",
        "import torch.nn.functional as F\n",
        "# 옵티마이저(optim) 모듈을 불러옴.\n",
        "from torch import optim\n",
        "\n",
        "# TorchVision 라이브러리를 불러옴.\n",
        "import torchvision\n",
        "# 이미지 변환(transforms) 모듈을 불러옴.\n",
        "from torchvision import transforms\n",
        "\n",
        "# PIL 라이브러리의 Image 모듈을 불러옴. 이미지 처리에 사용함.\n",
        "from PIL import Image\n",
        "# 순서가 보장되는 딕셔너리(OrderedDict) 클래스를 불러옴. (특정 구조 저장에 유용함)\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "uy7s3xvA-Hro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gram Matrix 계산하는 파이토치 모듈 정의\n",
        "class GramMatrix(nn.Module):\n",
        "  # 순전파 정의\n",
        "  def forward(self, input):\n",
        "    b,c,h,w = input.size()\n",
        "    # input : hidden layer에서 출력(lower layer ~ higher layer 저수준 ~ 고수준)한 feature map 에서 가져온 값\n",
        "    # >> 특징(feature) 추출한 값 = 스타일 표현 사용했으니\n",
        "    # 입력 텐서 배치(b), 채널(c), 높이(h), 너비(w) 크기 추출 (차원정보 추출)\n",
        "    # (2,64,32,32) b=2, c=64, h=32, w=32\n",
        "    # 배치(b) 2 : 2장의 이미지\n",
        "    # 채널(c) 64 : 64개의 feature map\n",
        "    # h(높이), w(너비) 픽셀\n",
        "    F = input.view(b, c, h*w)\n",
        "    # 4차원 >> 3차원 변경 (텐서를 배치 * 채널, 높이 * 너비) 형태로 평탄화(flatten)함\n",
        "    # 이 구현에서는 배치 차원 유지, 채널차원만 평탄화함 (b,c h*w)\n",
        "    # (2,64,32,32) >> (2,64,32*32) 2d 이미지를 1줄로 쭉 펴는 것\n",
        "    # 배치(2) 마다(각각의 이미지마다) 64개 채널(feature map) * 1024개 위치 값\n",
        "    # 채널 별(색상, 질감, ....)로 길이가 1024개 있는 벡터\n",
        "    # c * (h*w) >> 공간 차원을 펼친 feature map 생성\n",
        "    # c * (h*w), c * (h*w) 내적(inner product)하려면 차원을 맞추어 주기 위해 전치행렬이 필요해요\n",
        "    # 3 * (5*5), 3 * (5*5), 3*25 (3*25).T >> 3*25 25*3 = 3*3 행렬 (c,c)\n",
        "    # Gram Matrix  각 채널(특성) 끼리(색상, 질감..) 내적\n",
        "    # channel i = [x_i1, x_i2.....], channel j = [x_j1, x_j2.....] 이 두 벡터의 내적\n",
        "    # >> channel i와 channel j의 상관정도\n",
        "\n",
        "    G = torch.bmm(F, F.transpose(1,2))\n",
        "    # 배치 행렬 곱(bmm) 사용, gram matrix G 계산\n",
        "    # F와 F의 채널-공간 차원 전치(transpose) 곱해줘요 >> 결과의 크기 (b, c, c)\n",
        "    # 채널 1(색상), 채널 2(질감) 있다면, 채널 1과 채널 2가 얼마나 함께 활성화 되는가 계산\n",
        "    # 높으면 두 특징이 자주 함께 나타난다는 의미고 낮으면 두 특징이 잘 안 나타난다는 의미\n",
        "    # 채널 1.shape (b, n, m) 채널 2.shape (b, m, p)\n",
        "    # >> (b, n, p) >> 배치 차원 b 개에 대해 각각 행렬 곱 수행\n",
        "    # (batch_size, channel, h*w)\n",
        "\n",
        "    G.div_(h*w)\n",
        "    # gram matrix 를 높이 * 너비(h*w)로 나누어 정규화\n",
        "    # 왜 (높이*너비) 나눠요? 이미지 크기에 관계없이 일정한 값 갖게 하기 위해서\n",
        "    # 왜 해요? 값의 범위를 안정화하기 위해 (정규화 >> 안정)\n",
        "\n",
        "    return G"
      ],
      "metadata": {
        "id": "8D8WfVJC_LBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gram matrix 에 대해 MSE(평균 제곱 오차) 손실 계산하는 모듈 정의\n",
        "class GramMSELoss(nn.Module):\n",
        "  # 순전파(forward) 정의함. input: 현재 이미지의 특징, target: 목표로 하는 이미지 gram matrix\n",
        "  def forward(self, input, target):\n",
        "    out = nn.MSELoss()(GramMatrix()(input),target)\n",
        "    # GramMatrix 모듈 통과 > gram matrix 계산 >> 목표(target) gram matix와 비교\n",
        "    # >> MSE 손실 계산\n",
        "    # target 은 함수가 아니예요. (tensor) 데이터예요.\n",
        "    return (out)"
      ],
      "metadata": {
        "id": "8Xf30kNh_LEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "out = nn.MSELoss()(GramMatrix()(input),target)\n",
        "\n",
        "mse = nn.MSELoss() # 객체 생성\n",
        "gm_input = GramMatrix()(input)\n",
        "out = mse(gm_input, target)\n",
        "'''"
      ],
      "metadata": {
        "id": "RSoPTXoNSjia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content-style Loss"
      ],
      "metadata": {
        "id": "WvEMIjbzVriN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, pool='max'):\n",
        "        super(VGG, self).__init__()\n",
        "        #vgg modules\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        if pool == 'max':     # 특징이 뚜렷한 것만 추출해줘\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pool == 'avg':  # 특징을 평균해서 가져와(부드러운 특징)\n",
        "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, out_keys):\n",
        "        out = {}\n",
        "        out['r11'] = F.relu(self.conv1_1(x))\n",
        "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
        "        out['p1'] = self.pool1(out['r12'])\n",
        "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
        "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
        "        out['p2'] = self.pool2(out['r22'])\n",
        "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
        "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
        "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
        "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
        "        out['p3'] = self.pool3(out['r34'])\n",
        "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
        "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
        "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
        "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
        "        out['p4'] = self.pool4(out['r44'])\n",
        "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
        "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
        "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
        "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
        "        out['p5'] = self.pool5(out['r54'])\n",
        "        return [out[key] for key in out_keys]"
      ],
      "metadata": {
        "id": "iCp_Mnnw_LGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[VGG 구조 패턴]\n",
        "\n",
        "- Block 1: 3→64→64 (얕은 특징: 선, 모서리)\n",
        "- Block 2: 64→128→128 (중간 특징: 질감)\n",
        "- Block 3: 128→256→256→256 (깊은 특징: 패턴)\n",
        "- Block 4: 256→512→512→512 (더 복잡한 특징)\n",
        "- Block 5: 512→512→512→512 (추상적 특징)"
      ],
      "metadata": {
        "id": "DG_YlCkmWPaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG()\n",
        "\n",
        "# 로드할 이미지 파일 이름들을 정의함. ('vangogh_starry_night.jpg', 'Tuebingen_Neckarfront.jpg')\n",
        "img1 = \"/vangogh_starry_night.jpg\"\n",
        "img2 = \"/Tuebingen_Neckarfront.jpg\"\n",
        "\n",
        "\n",
        "# 이미지 디렉토리와 파일 이름을 결합하여 PIL Image 객체 리스트로 로드함.\n",
        "img1 = Image.open(img1)\n",
        "img2 = Image.open(img2)\n",
        "imgs = []\n",
        "imgs.append(img1)\n",
        "imgs.append(img2)\n",
        "\n",
        "img_size = 512\n",
        "prep = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),  # Tensor (chw) [0.1] 범위\n",
        "            transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), # RGB >> BGR 채널 순서변경\n",
        "            transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
        "                                                        std=[1,1,1]), # ImageNet  평균값 정규화\n",
        "            transforms.Lambda(lambda x: x.mul_(255)),\n",
        "            # pixel 범위 [0,255] 범위로 스케일 >> 픽셀 크기를 VGG 원래 입력 스케일에 맞춤\n",
        "        ])\n",
        "\n",
        "# 각 PIL Image 객체에 사전 정의된 전처리함수(prep)를 적용 >> 텐서로 변환\n",
        "imgs_torch = [prep(img) for img in imgs] # imgs 이미지 리스트 전체\n",
        "\n",
        "# GPU(CUDA) 사용이 가능하다면:\n",
        "if torch.cuda.is_available():\n",
        "    # 각 텐서에 배치 차원(unsqueeze(0))을 추가하고 GPU로 이동시킨 후, Variable로 감싸서 저장함.\n",
        "    imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
        "# GPU를 사용할 수 없다면:\n",
        "else:\n",
        "    # 각 텐서에 배치 차원만 추가하고 Variable로 감싸서 저장함.\n",
        "    imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
        "\n",
        "'''\n",
        "if torch.cuda.is_available():\n",
        "    # CUDA가 사용 가능하면, 텐서를 GPU로 이동 (.cuda() 또는 .to(device))\n",
        "    imgs_torch = [img.unsqueeze(0).cuda() for img in imgs_torch]\n",
        "else:\n",
        "    # CUDA를 사용할 수 없으면, 텐서를 CPU에 유지\n",
        "    imgs_torch = [img.unsqueeze(0) for img in imgs_torch]\n",
        "\n",
        "'''\n",
        "# 이미지 텐서 리스트를 스타일 이미지와 콘텐츠 이미지 변수에 할당\n",
        "style_image, content_image = imgs_torch\n",
        "\n",
        "opt_img = Variable(content_image.data.clone(), requires_grad=True)\n",
        "# 형태 유지하면서 스타일만 변형되도록 학습(경사하강법) 시작\n",
        "# >> 이미지 자체가 학습 대상(파라미터)\n",
        "# (**) 스타일 트랜스퍼(전이) CNN 가중치 학습은 하지 않아요. opt_img의 픽셀 값만 학습되면서 업데이트\n",
        "\n",
        "'''\n",
        "opt_img = content_image.clone().detach().requires_grad_(True)\n",
        "# content_image (데이터, 값) 복제해서 기존 그래프 분리(detach) >> 기울기 계산\n",
        "'''\n"
      ],
      "metadata": {
        "id": "QkLT5gKNWVPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# GramMSELoss와 vgg는 정의되어 있다고 가정합니다.\n",
        "# from your_modules import GramMSELoss, vgg, GramMatrix\n",
        "\n",
        "# 사용할 GPU를 설정하고, VGG 모델을 GPU로 이동합니다.\n",
        "if torch.cuda.is_available():\n",
        "    # VGG 모델을 GPU 메모리로 이동\n",
        "    vgg = vgg.cuda()\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# 스타일 손실을 계산할 VGG 레이어 이름 정의\n",
        "style_layers = ['r11','r21','r31','r41', 'r51']\n",
        "# style_layers 의미? VGG 내부의 특정 합성곱 레이어\n",
        "\n",
        "# 콘텐츠 손실을 계산할 VGG 레이어 이름 정의\n",
        "# (일반적으로 중간정도 레이어 중 하나 사용)\n",
        "content_layers = ['r42']\n",
        "\n",
        "loss_layers = style_layers + content_layers\n",
        "# 사용할 모든 손실 레이어를 스타일 레이어와 콘텐츠 에이어 합쳐 정의\n",
        "\n",
        "# 각 스타일에 대해 GramMSELoss 모듈을 사용하고, 콘텐츠에 대해 MSELoss 모듈을 사용하도록 리스트를 정의함.\n",
        "# GramMSELoss 모듈 : 스타일 손실(loss) 계산\n",
        "# [GramMSELoss, GramMSELoss, ..., nn.MSELoss] 형태가 됨.\n",
        "# GramMSELoss가 nn.Module을 상속한다고 가정합니다.\n",
        "\n",
        "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
        "\n",
        "# GPU (CUDA) 사용이 가능하다면, 모든 손실 함수 모듈을 GPU 메모리로 이동시킴.\n",
        "if torch.cuda.is_available():\n",
        "    # loss_fns 리스트의 요소들을 새로운 모듈 인스턴스로 만들고 .cuda()를 적용\n",
        "    # 리스트 복사를 방지하고 정확하게 GPU로 이동하기 위해 수정\n",
        "    loss_fns = [GramMSELoss().to(device) for _ in style_layers] + \\\n",
        "               [nn.MSELoss().to(device) for _ in content_layers]\n",
        "    #  for _ in style_layers : style layers 개수만큼 GramMSELoss() 넣어요 >> 인스턴스\n",
        "    #  >> 각각 새로 생성\n",
        "else:\n",
        "    # CPU 사용 시에도 동일한 로직으로 인스턴스화\n",
        "    loss_fns = [GramMSELoss().to(device) for _ in style_layers] + \\\n",
        "               [nn.MSELoss().to(device) for _ in content_layers]\n",
        "\n",
        "# 스타일 레이어 수만큼 스타일 손실 모듈 + 콘텐츠 레이어의 수 만큼 MSELoss 모듈 생심\n",
        "\n",
        "# 스타일 손실에 적용할 가중치(beta)를 정의\n",
        "# 깊은 레이어일수록(복잡한 패턴을 추출하는 레이어) 낮은 가중치를 주는 경향이 있음\n",
        "# 왜? 가중치가 감소되니깐\n",
        "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
        "# [64,128,256,512,512] : channel(c) 수\n",
        "# 1e3/n**2 : 보정 값. 깊은 레이어일 수로 feature 수가 많고 값도 커지므로 스타일 손실이 너무 커지는 거 방지\n",
        "\n",
        "\n",
        "# 콘텐츠 손실에 부여할 가중치(alpha)를 정의\n",
        "content_weights = [1e0]\n",
        "# 1e0 : 1\n",
        "\n",
        "weights = style_weights + content_weights\n",
        "\n",
        "# 최적화 목표값 (style targets) 계산\n",
        "# style_image >> VGG 통과 >> 각 스타일 레이어 Gram Matrix 계산\n",
        "# >> 변화도 추적에서 제외 (detach)\n",
        "style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
        "# vgg(style_image, style_layers) 지정한 레이어들의 특징맵(feature map )리스트로 반환\n",
        "# [A_r11, A_r21.....] 여기서 A.shape (b,c,h,w)\n",
        "# GramMatrix(A) >> 형태 변환 (b, c, c) >> 스타일 표현\n",
        "# .detach() 계산 그래프 분리(역전파시 gradient 계산되지 않도록)\n",
        "# >> 왜? style_targets 고정된 값(ground truth)\n",
        "# style_targets? 각 스타일 레이어에 대한 스타일 이미지 Gram Matrix 목록\n",
        "\n",
        "# 최적화 목표값(content targets)을 계산함.\n",
        "# 콘텐츠 이미지(content_image)를 VGG에 통과시켜 콘텐츠 레이어의 특징 맵을 추출하고 변화도 추적에서 제외(detach)했음.\n",
        "# content_image 또한 이미 .cuda() 또는 .to(device)로 GPU에 로드되어 있다고 가정합니다.\n",
        "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
        "\n",
        "# 최종적으로 사용할 모든 목표값 리스트를 정의함.\n",
        "targets = style_targets + content_targets\n"
      ],
      "metadata": {
        "id": "iXL7kg4GZCk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "id": "G0Tjz08czpRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = content_image.clone().requires_grad_(True)\n",
        "# input_image 는 VGG에 입력될 초기 이미지, 콘텐츠 이미지와 동일해야 함\n",
        "input_image"
      ],
      "metadata": {
        "collapsed": true,
        "id": "q-IgOjhH1tSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L-BFGS 옵티마이저: 최적화 단계에서 closure 함수 요구\n",
        "\n",
        "optimizer = torch.optim.LBFGS([input_image], max_iter=1)\n",
        "# input_image 초기 이미지(최적화 대상), CNN 가중치가 아니라, ***이미지 픽셀*** 학습\n",
        "\n",
        "# optimizer = torch.optim.Adam([input_image], lr=0.01)\n",
        "# Adam 사용 가능\n",
        "# 일반적 Adam(안정되면서도 빠른 테스트 결과물 기대)\n",
        "# cf.LBFGS (Quasi Newton optimizer): 고차원 연속 계산 적합 (고품질 결과물 기대)\n",
        "\n",
        "# 최적화 함수 세기 위한 변수\n",
        "n_iter = 0"
      ],
      "metadata": {
        "id": "LNO7EDSD2SPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closure():\n",
        "    global n_iter\n",
        "\n",
        "    # 이전 기울기를 초기화합니다.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 생성된 이미지(input_image)를 VGG에 통과시켜 특징 맵을 추출합니다.\n",
        "    # vgg는 이전에 정의되어 GPU로 이동되었다고 가정합니다.\n",
        "    out = vgg(input_image, loss_layers)\n",
        "    # input_image : 현재 생성된 이미지 >> vgg() >> feature map 목록\n",
        "    # loss_layers 에 해당하는 레이어의 feature map list 출력 [f_r11, f_r21,....]\n",
        "\n",
        "    # 총 손실을 계산합니다.\n",
        "    layer_losses = [] # 스타일/콘텐츠 손실 (개별적으로 저)\n",
        "    total_loss = 0\n",
        "\n",
        "    # 각 레이어 별로 콘텐츠 손실 또는 스타일 손실 계산\n",
        "    # >> 가중치 (weight) 곱해줘요\n",
        "    # 전첸 소실에 더해줘요\n",
        "    for i, weight in enumerate(weights):\n",
        "        target = targets[i] # 스타일 이미지 / 콘텐츠 이미지 target 값\n",
        "        feature = out[i]    # 현재 생성이미지의 feature map (특징맵)\n",
        "        loss_fn = loss_fns[i] # 해당 레이어 loss function (GramMSELoss, MSELoss)\n",
        "\n",
        "        # GramMatrix 계산이 필요한 스타일 레이어 처리 (GramMSELoss가 GramMatrix를 내부에서 처리한다고 가정)\n",
        "        # GramMatrix()가 별도 모듈이면, GramMSELoss 내부에 GramMatrix가 포함되어 있어야 합니다.\n",
        "\n",
        "        loss = weight * loss_fn(feature, target)\n",
        "        layer_losses.append(loss.item())\n",
        "        total_loss += loss\n",
        "\n",
        "    # 역전파를 수행하여 기울기를 계산합니다.\n",
        "    # gradinet 계산됨 >> input_image 에 대해 계산됨\n",
        "    total_loss.backward()\n",
        "\n",
        "    # 진행 상황을 출력합니다.\n",
        "    if n_iter % 50 == 0:\n",
        "        print(f\"Iteration {n_iter}: Total Loss = {total_loss.item():.4f}\")\n",
        "        # print(f\"Layer Losses: {layer_losses}\") # 디버깅용\n",
        "\n",
        "    n_iter += 1\n",
        "    return total_loss\n",
        "\n",
        "# 최적화 실행 (반복 횟수 지정)\n",
        "num_iterations = 500 # 원하는 반복 횟수를 설정합니다.\n",
        "for i in range(num_iterations):\n",
        "    # L-BFGS는 step() 호출 시마다 클로저를 여러 번 호출할 수 있습니다.\n",
        "    optimizer.step(closure)\n",
        "    # closure 반환 값을 기반으로 다음 업데이트 방향 계산\n",
        "\n",
        "    # 참고: Adam을 사용한다면, 루프는 다음과 같습니다.\n",
        "    # loss = closure()\n",
        "    # optimizer.step()\n",
        "\n",
        "# 최종 결과 이미지 후처리 (옵션) 이미지 픽셀값 정규화\n",
        "# 생성된 이미지를 [0, 1] 범위로 클리핑하여 픽셀 값을 보정합니다.\n",
        "input_image.data.clamp_(0, 1)\n",
        "\n",
        "# 최적화된 input_image.data가 최종 스타일 트랜스퍼 결과입니다."
      ],
      "metadata": {
        "id": "rS2yhLWz3MKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Style Transfer (스타일 전이)\n",
        "- target: 학습이 도달해야 할 기준 (reference 정답)  \n",
        "  - 생성되는 이미지(변하는 이미지) 따라가야 할 목표 상태(정답)\n",
        "\n",
        "- content 목표 / style 목표\n",
        "  - content 목표 : 이미지 속 형태나 구조 유지\n",
        "    - target 콘텐츠 이미지의 feature map\n",
        "  - style 목표: 이미지 가지고 있는 질감, 색감, 패턴 재현\n",
        "     - target 스타일 이미지의 Gram Matrix\n",
        "\n",
        "- 원본(style, content) 에서 추출된 features 정보\n",
        "  - 생성 이미지가 학습을 통해 그 값과 가까워지는 것\n",
        "  - loss가 0에 가까워 지는\n"
      ],
      "metadata": {
        "id": "fRbJ_MR19Oh2"
      }
    }
  ]
}